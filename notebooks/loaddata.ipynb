{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow_practice.config import settings\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 19:20:25 WARN Utils: Your hostname, LinnoMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.12 instead (on interface en0)\n",
      "24/12/18 19:20:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 19:20:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Generate a spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Set the spark session time zone of the timestamp column to utc\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd` stans for `Resilient Distributed Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定したフォルダー配下の全てのファイルと読み込む\n",
    "path = str(settings.data_path/\".raw\"/\"*\")\n",
    "rdd = spark.sparkContext.textFile(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23583 20200101 0005 20191231 1505  2.514 -158.61   59.28   -17.5     0.0     30 0   -21.8 C 0    60 0 -99.000 -9999.0  1015 0   1.59 0',\n",
       " '23583 20200101 0010 20191231 1510  2.514 -158.61   59.28   -17.5     0.0     35 0   -22.1 C 0    60 0 -99.000 -9999.0  1015 0   1.35 0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210816"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` stands for `MapReduce`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line: str): \n",
    "    \"\"\"A function to prase each singlge line read from the raw data.\"\"\"\n",
    "\n",
    "    f = line.split()\n",
    "    # wbanno? \n",
    "    wbanno = f[0]\n",
    "\n",
    "    # Generate data-time string, converting to utc time\n",
    "    dt = datetime.strptime(f[1]+f[2], \"%Y%m%d%H%M\")\n",
    "    dt = dt.replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    # Replace the outlier of temperature (-9999.9)\n",
    "    temperature  = None if f[8] == '-9999.0' else float(f[8])\n",
    "\n",
    "    # Parse a string line into a Row object\n",
    "    return Row(timestamp=dt,wbanno=wbanno, temperature=temperature )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the map method to apply the parse line function on each line\n",
    "# Get a series of Row objects\n",
    "rows: list[Row] = rdd.map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(timestamp=datetime.datetime(2020, 1, 1, 0, 5, tzinfo=datetime.timezone.utc), wbanno='23583', temperature=-17.5),\n",
       " Row(timestamp=datetime.datetime(2020, 1, 1, 0, 10, tzinfo=datetime.timezone.utc), wbanno='23583', temperature=-17.5)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, wbanno: string, temperature: double]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df using rdd\n",
    "df = rdd.map(parse_line).toDF()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------+\n",
      "|          timestamp|wbanno|temperature|\n",
      "+-------------------+------+-----------+\n",
      "|2020-01-01 00:05:00| 23583|      -17.5|\n",
      "|2020-01-01 00:10:00| 23583|      -17.5|\n",
      "+-------------------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 20:00:17 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 15 (TID 16): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the upper 2 lines: \n",
    "df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|           wbanno|       temperature|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|           210816|            207720|\n",
      "|   mean|          59994.0|2.7866825534372963|\n",
      "| stddev|36411.08635760458| 9.831826313535235|\n",
      "|    min|            23583|             -32.0|\n",
      "|    max|            96405|              26.6|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the aggressive information\n",
    "df.describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp view\n",
    "df.createOrReplaceTempView('uscrn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----+-------------------+-----+\n",
      "|wbanno|      timestamp_min|t_min|      timestamp_max|t_max|\n",
      "+------+-------------------+-----+-------------------+-----+\n",
      "| 23583|2020-02-01 16:15:00|-32.0|2020-08-17 00:20:00| 24.8|\n",
      "| 96405|2020-01-10 17:30:00|-25.6|2020-07-03 22:55:00| 26.6|\n",
      "+------+-------------------+-----+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use SQL query\n",
    "query = '''\n",
    "SELECT\n",
    "  wbanno,\n",
    "  min_by(timestamp, temperature) timestamp_min,\n",
    "  min(temperature) t_min,\n",
    "  max_by(timestamp, temperature) timestamp_max,\n",
    "  max(temperature) t_max\n",
    "FROM\n",
    "  uscrn\n",
    "GROUP by\n",
    "  1\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
